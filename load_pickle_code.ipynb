{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7b3a0l2ep4"
      },
      "source": [
        "1. mon_standard.pkl > array code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mfwrTwPtd36",
        "outputId": "22f73d36-41a3-4df6-86c9-74a28d36534f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datafile...\n",
            "Total samples: 1900\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "USE_SUBLABEL = False\n",
        "URL_PER_SITE = 10\n",
        "TOTAL_URLS   = 95\n",
        "\n",
        "# Load the pickle file\n",
        "print(\"Loading datafile...\")\n",
        "with open(\"mon_standard.pkl\", 'rb') as fi: # Path to mon_standard.pkl in Colab\n",
        "    data = pickle.load(fi)\n",
        "\n",
        "X1_mon = [] # Array to store instances (timestamps) - 19,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
        "X2_mon = [] # Array to store instances (direction*size) - size information\n",
        "y_mon = [] # Array to store the site of each instance - 19,000 instances, e.g., [0, 0, 0, 0, 0, 0, ..., 94, 94, 94, 94, 94]\n",
        "\n",
        "# Differentiate instances and sites, and store them in the respective x and y arrays\n",
        "# x array (direction*timestamp), y array (site label)\n",
        "for i in range(TOTAL_URLS):\n",
        "    if USE_SUBLABEL:\n",
        "        label = i\n",
        "    else:\n",
        "        label = i // URL_PER_SITE # Calculate which site's URL the current URL being processed belongs to and set that value as the label. Thus, URLs fetched from the same site are labeled identically.\n",
        "    for sample in data[i]:\n",
        "        size_seq = []\n",
        "        time_seq = []\n",
        "        for c in sample:\n",
        "            dr = 1 if c > 0 else -1\n",
        "            time_seq.append(abs(c))\n",
        "            size_seq.append(dr * 512)\n",
        "        X1_mon.append(time_seq)\n",
        "        X2_mon.append(size_seq)\n",
        "        y_mon.append(label)\n",
        "size = len(y_mon)\n",
        "\n",
        "print(f'Total samples: {size}') # Output: 19000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz5mat0w2dJy"
      },
      "source": [
        "2. unmon_standard10.pkl > array code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWfcIOZovSMl",
        "outputId": "21d76456-9043-4335-ff3a-7c8c1b89f25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datafile...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 3000\n",
            "300\n",
            "[-512 -512  512 ... -512 -512 -512]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "TOTAL_URLS = 300  # total number in the dataset\n",
        "\n",
        "# Load 10,000 unmon pickle file\n",
        "print(\"Loading datafile...\")\n",
        "with open('unmon_standard10_3000.pkl', 'rb') as f:  # Path to unmon_standard10.pkl in Colab\n",
        "    x = pickle.load(f)\n",
        "\n",
        "size = len(x)\n",
        "print(f'Total samples: {size}')\n",
        "\n",
        "X1_unmon = [] # Array to store instances (timestamps) - 10,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
        "X2_unmon = [] # Array to store instances (direction*size) - size information\n",
        "\n",
        "for i in range(TOTAL_URLS):\n",
        "    size_seq = []\n",
        "    time_seq = []\n",
        "    for c in x[i]:\n",
        "        dr = 1 if c > 0 else -1\n",
        "        time_seq.append(abs(c))\n",
        "        size_seq.append(dr * 512) # In the pickle file, there is no size information, so the conversion code is set to multiply by 512 uniformly.\n",
        "    X1_unmon.append(np.array(time_seq, dtype=np.int32))\n",
        "    X2_unmon.append(np.array(size_seq, dtype=np.int16))\n",
        "\n",
        "\n",
        "print(len(X1_unmon)) # Print the length of X1\n",
        "\n",
        "print(X2_unmon[0-10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBGa8u6_0pZ6"
      },
      "source": [
        "### Data Preprocessing ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBIIKY_i0pZ6"
      },
      "source": [
        "#### Remove corrupted/incomplete traces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4fBIcRD0pZ6",
        "outputId": "a598a24f-050a-4a03-a276-8c06cb5641c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean monitored traces: 1900\n",
            "Clean unmonitored traces: 300\n"
          ]
        }
      ],
      "source": [
        "def clean(X1, X2, y=None):\n",
        "    X1_clean, X2_clean, y_clean = [], [], []\n",
        "    for i in range(len(X1)):\n",
        "        if len(X1[i]) > 0 and len(X1[i]) == len(X2[i]): # non-empty & matching lengths\n",
        "            X1_clean.append(X1[i])\n",
        "            X2_clean.append(X2[i])\n",
        "            if y is not None:\n",
        "                y_clean.append(y[i])\n",
        "    return (X1_clean, X2_clean, y_clean) if y is not None else (X1_clean, X2_clean)\n",
        "\n",
        "# clean monitored\n",
        "X1_mon, X2_mon, y_mon = clean(X1_mon, X2_mon, y_mon)\n",
        "print(\"Clean monitored traces:\", len(X1_mon))\n",
        "\n",
        "# clean unmonitored\n",
        "X1_unmon, X2_unmon = clean(X1_unmon, X2_unmon)\n",
        "print(\"Clean unmonitored traces:\", len(X1_unmon))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSgGjkWI0pZ6"
      },
      "source": [
        "#### Normalize timestamps to start at 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "F8NVpIuk0pZ7"
      },
      "outputs": [],
      "source": [
        "def normalize_timestamps(X1):\n",
        "    return [[t - seq[0] for t in seq] for seq in X1] # subtract by first seq value for each value to see how much time passed in each packet\n",
        "\n",
        "X1_mon = normalize_timestamps(X1_mon)\n",
        "X1_unmon = normalize_timestamps(X1_unmon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYjNUhF70pZ7"
      },
      "source": [
        "#### Truncate or pad sequences to certain length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzfsgqRS0pZ7",
        "outputId": "659c643e-0e93-4f2b-9e5b-1576fbfbd0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Monitored timestamps: (1900, 10000)\n",
            "Unmonitored timestamps: (300, 10000)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "MAX_LEN = 10000\n",
        "\n",
        "def pad_truncate(seq, max_len=10000):\n",
        "    seq = list(seq)  # assure une liste Python\n",
        "\n",
        "    if len(seq) > max_len:\n",
        "        return seq[:max_len]\n",
        "\n",
        "    if len(seq) < max_len:\n",
        "        return seq + [0] * (max_len - len(seq))\n",
        "\n",
        "    return seq\n",
        "\n",
        "# --- MONITORED --------------------------------------------------------------\n",
        "X1_mon = np.array([pad_truncate(s, MAX_LEN) for s in X1_mon])\n",
        "X2_mon = np.array([pad_truncate(s, MAX_LEN) for s in X2_mon])\n",
        "y_mon = np.array(y_mon)\n",
        "\n",
        "# --- UNMONITORED ------------------------------------------------------------\n",
        "X1_unmon = np.array([pad_truncate(s, MAX_LEN) for s in X1_unmon])\n",
        "X2_unmon = np.array([pad_truncate(s, MAX_LEN) for s in X2_unmon])\n",
        "\n",
        "print(\"\\nMonitored timestamps:\", X1_mon.shape)\n",
        "print(\"Unmonitored timestamps:\", X1_unmon.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_L_u0Bh0pZ7"
      },
      "source": [
        "#### Split data into training, testing, and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQx8iY3v0pZ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1) SCENARIO CLOSED WORLD\n",
        "\n",
        "X1_train_cw, X1_temp_cw, X2_train_cw, X2_temp_cw, y_train_cw, y_temp_cw = train_test_split(\n",
        "    X1_mon, X2_mon, y_mon,\n",
        "    test_size=0.30,\n",
        "    stratify=y_mon,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X1_val_cw, X1_test_cw, X2_val_cw, X2_test_cw, y_val_cw, y_test_cw = train_test_split(\n",
        "    X1_temp_cw, X2_temp_cw, y_temp_cw,\n",
        "    test_size=0.50,\n",
        "    stratify=y_temp_cw,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2) SCENARIO OPEN WORLD (binary 0/1)\n",
        "\n",
        "y_mon_binary = np.ones(len(X1_mon))        # SurveillÃ© = 1\n",
        "y_unmon_binary = np.zeros(len(X1_unmon))   # Non-surveillÃ© = 0\n",
        "\n",
        "# Combine les donnÃ©es\n",
        "X1_all = np.vstack((X1_mon, X1_unmon))\n",
        "X2_all = np.vstack((X2_mon, X2_unmon))\n",
        "y_all  = np.concatenate((y_mon_binary, y_unmon_binary))\n",
        "\n",
        "# Split open world\n",
        "X1_train_ow, X1_temp_ow, X2_train_ow, X2_temp_ow, y_train_ow, y_temp_ow = train_test_split(\n",
        "    X1_all, X2_all, y_all,\n",
        "    test_size=0.30,\n",
        "    stratify=y_all,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X1_val_ow, X1_test_ow, X2_val_ow, X2_test_ow, y_val_ow, y_test_ow = train_test_split(\n",
        "    X1_temp_ow, X2_temp_ow, y_temp_ow,\n",
        "    test_size=0.50,\n",
        "    stratify=y_temp_ow,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **FEATURE EXTRACTION** (Alexandre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def extract_all_features(X1, X2):\n",
        "    all_features = []\n",
        "    \n",
        "    for idx in range(len(X1)):\n",
        "        # Remove padding (zeros)\n",
        "        time_seq = X1[idx][X1[idx] != 0]\n",
        "        size_seq = X2[idx][X2[idx] != 0]\n",
        "        \n",
        "        # Handle empty sequences\n",
        "        if len(time_seq) == 0 or len(size_seq) == 0:\n",
        "            # Return zero features for empty sequences\n",
        "            all_features.append([0] * 80)\n",
        "            continue\n",
        "        \n",
        "        feature_dict = {}\n",
        "        \n",
        "        # ====================================================================\n",
        "        # 1. STATISTICAL FEATURES\n",
        "        # ====================================================================\n",
        "        \n",
        "        # Basic counts\n",
        "        feature_dict['total_packets'] = len(size_seq)\n",
        "        feature_dict['incoming_packets'] = np.sum(size_seq < 0)\n",
        "        feature_dict['outgoing_packets'] = np.sum(size_seq > 0)\n",
        "        \n",
        "        # Byte statistics\n",
        "        feature_dict['total_bytes_in'] = np.sum(np.abs(size_seq[size_seq < 0]))\n",
        "        feature_dict['total_bytes_out'] = np.sum(size_seq[size_seq > 0])\n",
        "        feature_dict['total_bytes'] = np.sum(np.abs(size_seq))\n",
        "        \n",
        "        # Packet size statistics\n",
        "        abs_sizes = np.abs(size_seq)\n",
        "        feature_dict['mean_packet_size'] = np.mean(abs_sizes)\n",
        "        feature_dict['std_packet_size'] = np.std(abs_sizes)\n",
        "        feature_dict['median_packet_size'] = np.median(abs_sizes)\n",
        "        feature_dict['max_packet_size'] = np.max(abs_sizes)\n",
        "        feature_dict['min_packet_size'] = np.min(abs_sizes)\n",
        "        \n",
        "        # Ratios\n",
        "        feature_dict['in_out_packet_ratio'] = (feature_dict['incoming_packets'] / max(feature_dict['outgoing_packets'], 1))\n",
        "        feature_dict['in_out_byte_ratio'] = (feature_dict['total_bytes_in'] / max(feature_dict['total_bytes_out'], 1))\n",
        "        \n",
        "        # ====================================================================\n",
        "        # 2. BURST FEATURES\n",
        "        # ====================================================================\n",
        "        \n",
        "        # Identify bursts (consecutive packets in same direction)\n",
        "        directions = np.sign(size_seq)\n",
        "        direction_changes = np.concatenate([[True], np.diff(directions) != 0])\n",
        "        burst_indices = np.where(direction_changes)[0]\n",
        "        \n",
        "        # Split into bursts\n",
        "        bursts = np.split(size_seq, burst_indices[1:])\n",
        "        burst_times = np.split(time_seq, burst_indices[1:])\n",
        "        \n",
        "        # Burst lengths (number of packets per burst)\n",
        "        burst_lengths = [len(b) for b in bursts]\n",
        "        \n",
        "        # Burst sizes (total bytes per burst)\n",
        "        burst_sizes = [np.sum(np.abs(b)) for b in bursts]\n",
        "        \n",
        "        # Burst durations (time span)\n",
        "        burst_durations = []\n",
        "        for bt in burst_times:\n",
        "            if len(bt) > 1:\n",
        "                burst_durations.append(bt[-1] - bt[0])\n",
        "            else:\n",
        "                burst_durations.append(0)\n",
        "        \n",
        "        # Burst statistics\n",
        "        feature_dict['num_bursts'] = len(bursts)\n",
        "        feature_dict['max_burst_length'] = np.max(burst_lengths) if burst_lengths else 0\n",
        "        feature_dict['avg_burst_length'] = np.mean(burst_lengths) if burst_lengths else 0\n",
        "        feature_dict['std_burst_length'] = np.std(burst_lengths) if len(burst_lengths) > 1 else 0\n",
        "        \n",
        "        feature_dict['max_burst_size'] = np.max(burst_sizes) if burst_sizes else 0\n",
        "        feature_dict['avg_burst_size'] = np.mean(burst_sizes) if burst_sizes else 0\n",
        "        feature_dict['std_burst_size'] = np.std(burst_sizes) if len(burst_sizes) > 1 else 0\n",
        "        \n",
        "        feature_dict['max_burst_duration'] = np.max(burst_durations) if burst_durations else 0\n",
        "        feature_dict['avg_burst_duration'] = np.mean(burst_durations) if burst_durations else 0\n",
        "        \n",
        "        # Incoming vs outgoing bursts\n",
        "        incoming_bursts = [b for b in bursts if b[0] < 0]\n",
        "        outgoing_bursts = [b for b in bursts if b[0] > 0]\n",
        "        \n",
        "        feature_dict['num_incoming_bursts'] = len(incoming_bursts)\n",
        "        feature_dict['num_outgoing_bursts'] = len(outgoing_bursts)\n",
        "        \n",
        "        # ====================================================================\n",
        "        # 3. TIMING FEATURES\n",
        "        # ====================================================================\n",
        "        \n",
        "        # Session duration\n",
        "        feature_dict['session_duration'] = time_seq[-1] - time_seq[0]\n",
        "        \n",
        "        # Inter-packet delays\n",
        "        if len(time_seq) > 1:\n",
        "            inter_packet_delays = np.diff(time_seq)\n",
        "            \n",
        "            feature_dict['mean_delay'] = np.mean(inter_packet_delays)\n",
        "            feature_dict['std_delay'] = np.std(inter_packet_delays)\n",
        "            feature_dict['median_delay'] = np.median(inter_packet_delays)\n",
        "            feature_dict['min_delay'] = np.min(inter_packet_delays)\n",
        "            feature_dict['max_delay'] = np.max(inter_packet_delays)\n",
        "            \n",
        "            # Percentiles\n",
        "            feature_dict['percentile_10_delay'] = np.percentile(inter_packet_delays, 10)\n",
        "            feature_dict['percentile_25_delay'] = np.percentile(inter_packet_delays, 25)\n",
        "            feature_dict['percentile_50_delay'] = np.percentile(inter_packet_delays, 50)\n",
        "            feature_dict['percentile_75_delay'] = np.percentile(inter_packet_delays, 75)\n",
        "            feature_dict['percentile_90_delay'] = np.percentile(inter_packet_delays, 90)\n",
        "            \n",
        "            # Time to first incoming packet\n",
        "            first_incoming_idx = np.where(size_seq < 0)[0]\n",
        "            if len(first_incoming_idx) > 0:\n",
        "                feature_dict['time_to_first_incoming'] = time_seq[first_incoming_idx[0]]\n",
        "            else:\n",
        "                feature_dict['time_to_first_incoming'] = 0\n",
        "        else:\n",
        "            # Single packet - set timing features to 0\n",
        "            for k in ['mean_delay', 'std_delay', 'median_delay', 'min_delay', 'max_delay',\n",
        "                      'percentile_10_delay', 'percentile_25_delay', 'percentile_50_delay',\n",
        "                      'percentile_75_delay', 'percentile_90_delay', 'time_to_first_incoming']:\n",
        "                feature_dict[k] = 0\n",
        "        \n",
        "        # ====================================================================\n",
        "        # 4. DIRECTIONAL PATTERN FEATURES\n",
        "        # ====================================================================\n",
        "        \n",
        "        # Number of direction changes\n",
        "        feature_dict['direction_changes'] = np.sum(direction_changes) - 1  # -1 because first is always True\n",
        "        \n",
        "        # Average length of unidirectional sequences (same as avg burst length)\n",
        "        feature_dict['avg_unidirectional_length'] = feature_dict['avg_burst_length']\n",
        "        \n",
        "        # First outgoing burst characteristics\n",
        "        if len(outgoing_bursts) > 0:\n",
        "            feature_dict['first_outgoing_burst_size'] = np.sum(outgoing_bursts[0])\n",
        "            feature_dict['first_outgoing_burst_length'] = len(outgoing_bursts[0])\n",
        "        else:\n",
        "            feature_dict['first_outgoing_burst_size'] = 0\n",
        "            feature_dict['first_outgoing_burst_length'] = 0\n",
        "        \n",
        "        # Traffic concentration (packets in first/last 20%)\n",
        "        split_20 = len(size_seq) // 5\n",
        "        split_80 = (4 * len(size_seq)) // 5\n",
        "        \n",
        "        feature_dict['packets_first_20_percent'] = split_20\n",
        "        feature_dict['packets_last_20_percent'] = len(size_seq) - split_80\n",
        "        \n",
        "        # Ratio of outgoing to incoming in different segments\n",
        "        # Beginning (first 30%)\n",
        "        split_30 = len(size_seq) // 3\n",
        "        early_out = np.sum(size_seq[:split_30] > 0)\n",
        "        early_in = np.sum(size_seq[:split_30] < 0)\n",
        "        feature_dict['early_out_in_ratio'] = early_out / max(early_in, 1)\n",
        "        \n",
        "        # Middle (30%-70%)\n",
        "        split_70 = (7 * len(size_seq)) // 10\n",
        "        mid_out = np.sum(size_seq[split_30:split_70] > 0)\n",
        "        mid_in = np.sum(size_seq[split_30:split_70] < 0)\n",
        "        feature_dict['mid_out_in_ratio'] = mid_out / max(mid_in, 1)\n",
        "        \n",
        "        # End (last 30%)\n",
        "        late_out = np.sum(size_seq[split_70:] > 0)\n",
        "        late_in = np.sum(size_seq[split_70:] < 0)\n",
        "        feature_dict['late_out_in_ratio'] = late_out / max(late_in, 1)\n",
        "        \n",
        "        # ====================================================================\n",
        "        # 5. SEQUENCE-BASED FEATURES\n",
        "        # ====================================================================\n",
        "        \n",
        "        # First K packet directions (K=20)\n",
        "        K = 20\n",
        "        first_k_directions = directions[:min(K, len(directions))]\n",
        "        \n",
        "        # Count patterns in first K\n",
        "        feature_dict['first_k_incoming'] = np.sum(first_k_directions == -1)\n",
        "        feature_dict['first_k_outgoing'] = np.sum(first_k_directions == 1)\n",
        "        \n",
        "        # N-grams of directional patterns (bigrams and trigrams)\n",
        "        # Convert directions to string for n-gram extraction\n",
        "        dir_string = ''.join(['O' if d > 0 else 'I' for d in directions])\n",
        "        \n",
        "        # Bigrams (pairs)\n",
        "        if len(dir_string) >= 2:\n",
        "            bigrams = [dir_string[i:i+2] for i in range(len(dir_string)-1)]\n",
        "            bigram_counts = Counter(bigrams)\n",
        "            \n",
        "            feature_dict['bigram_OO'] = bigram_counts.get('OO', 0)\n",
        "            feature_dict['bigram_OI'] = bigram_counts.get('OI', 0)\n",
        "            feature_dict['bigram_IO'] = bigram_counts.get('IO', 0)\n",
        "            feature_dict['bigram_II'] = bigram_counts.get('II', 0)\n",
        "        else:\n",
        "            feature_dict['bigram_OO'] = 0\n",
        "            feature_dict['bigram_OI'] = 0\n",
        "            feature_dict['bigram_IO'] = 0\n",
        "            feature_dict['bigram_II'] = 0\n",
        "        \n",
        "        # Trigrams (triples)\n",
        "        if len(dir_string) >= 3:\n",
        "            trigrams = [dir_string[i:i+3] for i in range(len(dir_string)-2)]\n",
        "            trigram_counts = Counter(trigrams)\n",
        "            \n",
        "            feature_dict['trigram_OOO'] = trigram_counts.get('OOO', 0)\n",
        "            feature_dict['trigram_III'] = trigram_counts.get('III', 0)\n",
        "            feature_dict['trigram_OOI'] = trigram_counts.get('OOI', 0)\n",
        "            feature_dict['trigram_IOO'] = trigram_counts.get('IOO', 0)\n",
        "        else:\n",
        "            feature_dict['trigram_OOO'] = 0\n",
        "            feature_dict['trigram_III'] = 0\n",
        "            feature_dict['trigram_OOI'] = 0\n",
        "            feature_dict['trigram_IOO'] = 0\n",
        "        \n",
        "        if feature_dict['session_duration'] > 0:\n",
        "            num_windows = 10\n",
        "            window_size = feature_dict['session_duration'] / num_windows\n",
        "            cumulative_bytes = np.zeros(num_windows)\n",
        "            \n",
        "            for i, t in enumerate(time_seq):\n",
        "                window_idx = min(int(t / window_size), num_windows - 1)\n",
        "                cumulative_bytes[window_idx] += abs(size_seq[i])\n",
        "            \n",
        "            # Normalize cumulative bytes\n",
        "            if np.sum(cumulative_bytes) > 0:\n",
        "                cumulative_bytes = cumulative_bytes / np.sum(cumulative_bytes)\n",
        "            \n",
        "            for i in range(num_windows):\n",
        "                feature_dict[f'cumulative_bytes_window_{i}'] = cumulative_bytes[i]\n",
        "        else:\n",
        "            for i in range(10):\n",
        "                feature_dict[f'cumulative_bytes_window_{i}'] = 0\n",
        "        \n",
        "        # Convert to list\n",
        "        all_features.append(list(feature_dict.values()))\n",
        "    \n",
        "    # Get feature names from the first non-empty sample\n",
        "    feature_names = list(feature_dict.keys())\n",
        "    \n",
        "    return np.array(all_features), feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Extracting features...\")\n",
        "\n",
        "# CLOSED WORLD\n",
        "X_train_cw_features, feature_names = extract_all_features(X1_train_cw, X2_train_cw)\n",
        "X_val_cw_features, _ = extract_all_features(X1_val_cw, X2_val_cw)\n",
        "X_test_cw_features, _ = extract_all_features(X1_test_cw, X2_test_cw)\n",
        "\n",
        "# OPEN WORLD\n",
        "X_train_ow_features, _ = extract_all_features(X1_train_ow, X2_train_ow)\n",
        "X_val_ow_features, _ = extract_all_features(X1_val_ow, X2_val_ow)\n",
        "X_test_ow_features, _ = extract_all_features(X1_test_ow, X2_test_ow)\n",
        "\n",
        "print(f\"âœ“ {len(feature_names)} features extracted\")\n",
        "\n",
        "# SCALING\n",
        "scaler_cw = StandardScaler()\n",
        "X_train_cw_scaled = scaler_cw.fit_transform(X_train_cw_features)\n",
        "X_val_cw_scaled = scaler_cw.transform(X_val_cw_features)\n",
        "X_test_cw_scaled = scaler_cw.transform(X_test_cw_features)\n",
        "\n",
        "scaler_ow = StandardScaler()\n",
        "X_train_ow_scaled = scaler_ow.fit_transform(X_train_ow_features)\n",
        "X_val_ow_scaled = scaler_ow.transform(X_val_ow_features)\n",
        "X_test_ow_scaled = scaler_ow.transform(X_test_ow_features)\n",
        "\n",
        "print(\"âœ“ Features scaled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch2OOgLljO_w"
      },
      "source": [
        "## **KNN MODEL** (Alice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8T1gciYjbAd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Closed World Scaler ---\n",
        "scaler_cw = StandardScaler()\n",
        "\n",
        "# Fit only on CW training features\n",
        "scaler_cw.fit(X_train_cw_features)\n",
        "\n",
        "# Transform\n",
        "X_train_cw_scaled = scaler_cw.transform(X_train_cw_features)\n",
        "X_val_cw_scaled   = scaler_cw.transform(X_val_cw_features)\n",
        "X_test_cw_scaled  = scaler_cw.transform(X_test_cw_features)\n",
        "\n",
        "# --- Open World Scaler ---\n",
        "scaler_ow = StandardScaler()\n",
        "\n",
        "# Fit only on OW training features\n",
        "scaler_ow.fit(X_train_ow_features)\n",
        "\n",
        "# Transform\n",
        "X_train_ow_scaled = scaler_ow.transform(X_train_ow_features)\n",
        "X_val_ow_scaled   = scaler_ow.transform(X_val_ow_features)\n",
        "X_test_ow_scaled  = scaler_ow.transform(X_test_ow_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic-BnnRBoxlc",
        "outputId": "0f38c257-580a-40a4-c467-15f4c9148c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CLOSED WORLD====\n",
            "Train accuracy Â  Â  Â  : 0.7894736842105263\n",
            "Validation accuracy Â : 0.6736842105263158\n",
            "Test accuracy Â  Â  Â  Â : 0.6701754385964912\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"=== CLOSED WORLD====\")\n",
        "\n",
        "# KNN model\n",
        "knn_cw = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
        "\n",
        "# Train\n",
        "knn_cw.fit(X_train_cw_scaled, y_train_cw)\n",
        "\n",
        "# Validation accuracy\n",
        "y_train_pred_cw = knn_cw.predict(X_train_cw_scaled)\n",
        "y_val_pred_cw  = knn_cw.predict(X_val_cw_scaled)\n",
        "y_test_pred_cw = knn_cw.predict(X_test_cw_scaled)\n",
        "\n",
        "\n",
        "# Test accuracy\n",
        "train_acc_cw = accuracy_score(y_train_cw, y_train_pred_cw)\n",
        "val_acc_cw   = accuracy_score(y_val_cw, y_val_pred_cw)\n",
        "test_acc_cw  = accuracy_score(y_test_cw, y_test_pred_cw)\n",
        "\n",
        "\n",
        "print(\"Train accuracy Â  Â  Â  :\", train_acc_cw)\n",
        "print(\"Validation accuracy Â :\", val_acc_cw)\n",
        "print(\"Test accuracy Â  Â  Â  Â :\", test_acc_cw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Closed world K-NN Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- CLOSED WORLD K-NN OPTIMIZATION (GRID SEARCH) ---\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "\n",
            "Best parameters found    : {'n_neighbors': 3, 'weights': 'distance'}\n",
            "Accuracy WITHOUT tuning  : 0.6737\n",
            "Accuracy WITH tuning     : 0.6807\n",
            "IMPROVEMENT: +0.0070\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"\\n--- CLOSED WORLD K-NN OPTIMIZATION (GRID SEARCH) ---\")\n",
        "\n",
        "# 1. Define parameters to test\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],       # Test different neighbors\n",
        "    'weights': ['uniform', 'distance'] # Test weight modes\n",
        "}\n",
        "\n",
        "# 2. Run the search (cv=3 for faster execution)\n",
        "grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_knn.fit(X_train_cw_scaled, y_train_cw)\n",
        "\n",
        "# 3. Retrieve the best model\n",
        "best_knn_cw = grid_knn.best_estimator_\n",
        "tuned_acc_cw = best_knn_cw.score(X_test_cw_scaled, y_test_cw)\n",
        "\n",
        "print(f\"\\nBest parameters found    : {grid_knn.best_params_}\")\n",
        "print(f\"Accuracy WITHOUT tuning  : {test_acc_cw:.4f}\")\n",
        "print(f\"Accuracy WITH tuning     : {tuned_acc_cw:.4f}\")\n",
        "\n",
        "# 4. Display the gain\n",
        "gain = tuned_acc_cw - test_acc_cw\n",
        "if gain > 0:\n",
        "    print(f\"IMPROVEMENT: +{gain:.4f}\")\n",
        "else:\n",
        "    print(f\"ðŸ”¹ No significant improvement (default model was already good).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRk8pir-pjjG",
        "outputId": "0ed3cbfe-efaa-468e-8cbb-54ad929070ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== OPEN WORLD (binary 0=unmon, 1=mon) ===\n",
            "Train accuracy Â  Â  Â  : 0.9227272727272727\n",
            "Validation accuracy Â : 0.8757575757575757\n",
            "Test accuracy Â  Â  Â  Â : 0.8636363636363636\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== OPEN WORLD (binary 0=unmon, 1=mon) ===\")\n",
        "\n",
        "# KNN model\n",
        "knn_ow = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
        "\n",
        "# Train\n",
        "knn_ow.fit(X_train_ow_scaled, y_train_ow)\n",
        "\n",
        "y_train_pred_ow = knn_ow.predict(X_train_ow_scaled)\n",
        "y_val_pred_ow   = knn_ow.predict(X_val_ow_scaled)\n",
        "y_test_pred_ow  = knn_ow.predict(X_test_ow_scaled)\n",
        "\n",
        "# --- Calcul accuracy ---\n",
        "train_acc_ow = accuracy_score(y_train_ow, y_train_pred_ow)\n",
        "val_acc_ow   = accuracy_score(y_val_ow, y_val_pred_ow)\n",
        "test_acc_ow  = accuracy_score(y_test_ow, y_test_pred_ow)\n",
        "\n",
        "\n",
        "print(\"Train accuracy Â  Â  Â  :\", train_acc_ow)\n",
        "print(\"Validation accuracy Â :\", val_acc_ow)\n",
        "print(\"Test accuracy Â  Â  Â  Â :\", test_acc_ow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Open world K-NN Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- OPEN WORLD K-NN OPTIMIZATION (GRID SEARCH) ---\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "\n",
            "Best parameters found    : {'n_neighbors': 9, 'weights': 'uniform'}\n",
            "Accuracy WITHOUT tuning  : 0.8636\n",
            "Accuracy WITH tuning     : 0.8909\n",
            "IMPROVEMENT: +0.0273\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"\\n--- OPEN WORLD K-NN OPTIMIZATION (GRID SEARCH) ---\")\n",
        "\n",
        "# 1. Define parameters to test\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],       # Test different neighbors\n",
        "    'weights': ['uniform', 'distance'] # Test weight modes\n",
        "}\n",
        "\n",
        "# 2. Run the search (cv=3 for faster execution)\n",
        "# Note: We use the Open World variables (_ow) here\n",
        "grid_knn_ow = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_knn_ow.fit(X_train_ow_scaled, y_train_ow)\n",
        "\n",
        "# 3. Retrieve the best model\n",
        "best_knn_ow = grid_knn_ow.best_estimator_\n",
        "tuned_acc_ow = best_knn_ow.score(X_test_ow_scaled, y_test_ow)\n",
        "\n",
        "print(f\"\\nBest parameters found    : {grid_knn_ow.best_params_}\")\n",
        "print(f\"Accuracy WITHOUT tuning  : {test_acc_ow:.4f}\")\n",
        "print(f\"Accuracy WITH tuning     : {tuned_acc_ow:.4f}\")\n",
        "\n",
        "# 4. Display the gain\n",
        "gain_ow = tuned_acc_ow - test_acc_ow\n",
        "if gain_ow > 0:\n",
        "    print(f\"IMPROVEMENT: +{gain_ow:.4f}\")\n",
        "else:\n",
        "    print(f\"ðŸ”¹ No significant improvement (default model was already good).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtKCm2PSvs4Q"
      },
      "source": [
        "## **SVM (Alice)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jufyOJGvzj4",
        "outputId": "ef5d3519-77ce-4f94-94a3-7c5bbad79c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CLOSED WORLD (SVM) ====\n",
            "DÃ©marrage de l'entraÃ®nement...\n",
            "[LibSVM]Temps d'entraÃ®nement: 13.34 secondes\n",
            "\n",
            "--- RÃ‰SULTATS ---\n",
            "Train accuracy Â  Â  Â  : 0.8639097744360902\n",
            "Validation accuracy Â : 0.6456140350877193\n",
            "Test accuracy Â  Â  Â  Â : 0.6736842105263158\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "print(\"=== CLOSED WORLD (SVM) ====\")\n",
        "\n",
        "svm_cw = SVC(\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    C=1.0,\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --- Training ---\n",
        "start_time = time.time()\n",
        "print(\"DÃ©marrage de l'entraÃ®nement...\")\n",
        "svm_cw.fit(X_train_cw_scaled, y_train_cw)\n",
        "end_time = time.time()\n",
        "print(f\"Temps d'entraÃ®nement: {end_time - start_time:.2f} secondes\")\n",
        "\n",
        "# --- Predictions ---\n",
        "y_train_pred_cw = svm_cw.predict(X_train_cw_scaled) # Training\n",
        "y_val_pred_cw   = svm_cw.predict(X_val_cw_scaled)   # Validation\n",
        "y_test_pred_cw  = svm_cw.predict(X_test_cw_scaled)  # Test\n",
        "\n",
        "# --- Calcul accuracy ---\n",
        "train_acc_cw = accuracy_score(y_train_cw, y_train_pred_cw)\n",
        "val_acc_cw   = accuracy_score(y_val_cw, y_val_pred_cw)\n",
        "test_acc_cw  = accuracy_score(y_test_cw, y_test_pred_cw)\n",
        "\n",
        "\n",
        "print(\"\\n--- RÃ‰SULTATS ---\")\n",
        "print(\"Train accuracy Â  Â  Â  :\", train_acc_cw)\n",
        "print(\"Validation accuracy Â :\", val_acc_cw)\n",
        "print(\"Test accuracy Â  Â  Â  Â :\", test_acc_cw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Closed world SVM Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- CLOSED WORLD SVM OPTIMIZATION (GRID SEARCH) ---\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "\n",
            "Best parameters found    : {'C': 10, 'kernel': 'rbf'}\n",
            "Accuracy WITHOUT tuning  : 0.6737\n",
            "Accuracy WITH tuning     : 0.8526\n",
            "IMPROVEMENT: +0.1789\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"\\n--- CLOSED WORLD SVM OPTIMIZATION (GRID SEARCH) ---\")\n",
        "\n",
        "# 1. Define parameters to test\n",
        "# We limit the grid because SVM is computationally expensive on large data\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100], \n",
        "    'kernel': ['rbf'] \n",
        "}\n",
        "\n",
        "# 2. Run the search\n",
        "grid_svm_cw = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_svm_cw.fit(X_train_cw_scaled, y_train_cw)\n",
        "\n",
        "# 3. Retrieve the best model\n",
        "best_svm_cw = grid_svm_cw.best_estimator_\n",
        "tuned_acc_cw = best_svm_cw.score(X_test_cw_scaled, y_test_cw)\n",
        "\n",
        "print(f\"\\nBest parameters found    : {grid_svm_cw.best_params_}\")\n",
        "print(f\"Accuracy WITHOUT tuning  : {test_acc_cw:.4f}\")\n",
        "print(f\"Accuracy WITH tuning     : {tuned_acc_cw:.4f}\")\n",
        "\n",
        "# 4. Display the gain\n",
        "gain_cw = tuned_acc_cw - test_acc_cw\n",
        "if gain_cw > 0:\n",
        "    print(f\"IMPROVEMENT: +{gain_cw:.4f}\")\n",
        "else:\n",
        "    print(f\"No significant improvement.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "misZ-ZedwB6F",
        "outputId": "ead9579d-e446-401e-f0b7-1f81642e6bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OPEN WORLD (SVM) ====\n",
            "DÃ©marrage de l'entraÃ®nement...\n",
            "[LibSVM]Temps d'entraÃ®nement: 43.35 secondes\n",
            "\n",
            "--- RÃ‰SULTATS ---\n",
            "Train accuracy Â  Â  Â  : 0.8974025974025974\n",
            "Validation accuracy Â : 0.8666666666666667\n",
            "Test accuracy Â  Â  Â  Â : 0.8636363636363636\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "print(\"=== OPEN WORLD (SVM) ====\")\n",
        "\n",
        "svm_ow = SVC(\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    C=1.0,\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --- Training ---\n",
        "start_time = time.time()\n",
        "print(\"DÃ©marrage de l'entraÃ®nement...\")\n",
        "svm_ow.fit(X_train_ow_scaled, y_train_ow)\n",
        "end_time = time.time()\n",
        "print(f\"Temps d'entraÃ®nement: {end_time - start_time:.2f} secondes\")\n",
        "\n",
        "# --- Predictions ---\n",
        "y_train_pred_ow = svm_ow.predict(X_train_ow_scaled) # Training\n",
        "y_val_pred_ow   = svm_ow.predict(X_val_ow_scaled)   # Validation\n",
        "y_test_pred_ow  = svm_ow.predict(X_test_ow_scaled)  # Test\n",
        "\n",
        "# --- Calcul accuracy ---\n",
        "train_acc_ow = accuracy_score(y_train_ow, y_train_pred_ow)\n",
        "val_acc_ow   = accuracy_score(y_val_ow, y_val_pred_ow)\n",
        "test_acc_ow  = accuracy_score(y_test_ow, y_test_pred_ow)\n",
        "\n",
        "\n",
        "print(\"\\n--- RÃ‰SULTATS ---\")\n",
        "print(\"Train accuracy Â  Â  Â  :\", train_acc_ow)\n",
        "print(\"Validation accuracy Â :\", val_acc_ow)\n",
        "print(\"Test accuracy Â  Â  Â  Â :\", test_acc_ow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Open world SVM Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- OPEN WORLD SVM OPTIMIZATION (GRID SEARCH) ---\n",
            "Warning: This process might be slow...\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "\n",
            "Best parameters found    : {'C': 10, 'kernel': 'rbf'}\n",
            "Accuracy WITHOUT tuning  : 0.8636\n",
            "Accuracy WITH tuning     : 0.9333\n",
            "IMPROVEMENT: +0.0697\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"\\n--- OPEN WORLD SVM OPTIMIZATION (GRID SEARCH) ---\")\n",
        "print(\"Warning: This process might be slow...\")\n",
        "\n",
        "# 1. Define parameters to test\n",
        "# We use a limited grid to keep computation time reasonable\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],   # Regularization parameter\n",
        "    'kernel': ['rbf']    # RBF is usually best for this task\n",
        "}\n",
        "\n",
        "# 2. Run the search\n",
        "# cv=3 is used for speed. n_jobs=-1 uses all CPU cores.\n",
        "grid_svm_ow = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_svm_ow.fit(X_train_ow_scaled, y_train_ow)\n",
        "\n",
        "# 3. Retrieve the best model\n",
        "best_svm_ow = grid_svm_ow.best_estimator_\n",
        "tuned_acc_ow = best_svm_ow.score(X_test_ow_scaled, y_test_ow)\n",
        "\n",
        "print(f\"\\nBest parameters found    : {grid_svm_ow.best_params_}\")\n",
        "print(f\"Accuracy WITHOUT tuning  : {test_acc_ow:.4f}\")\n",
        "print(f\"Accuracy WITH tuning     : {tuned_acc_ow:.4f}\")\n",
        "\n",
        "# 4. Display the gain\n",
        "gain_ow = tuned_acc_ow - test_acc_ow\n",
        "if gain_ow > 0:\n",
        "    print(f\"IMPROVEMENT: +{gain_ow:.4f}\")\n",
        "else:\n",
        "    print(f\"No significant improvement.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
